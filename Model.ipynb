{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combining Features and Building Predictive Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this is for my (Eric) Spark setup, you don't need to run this\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.types as typ\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import col, asc\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType, DoubleType, DateType\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.mllib.regression as reg\n",
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from pyspark.mllib.tree import RandomForest, GradientBoostedTrees\n",
    "from pyspark.ml.stat import Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house = spark.read.csv('df_house.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up some titles\n",
    "df_house = df_house.withColumnRenamed('candidatevotes', 'CAND_VOTES')\n",
    "df_house = df_house.withColumnRenamed('totalvotes', 'TOTAL_VOTES')\n",
    "df_house = df_house.withColumnRenamed('VOTE_percent', 'PERCENT_VOTES')\n",
    "\n",
    "#drop index that is brought in\n",
    "df_house = df_house.drop(col('_c0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CAND_ID: string (nullable = true)\n",
      " |-- CAND_NAME: string (nullable = true)\n",
      " |-- CAND_PTY_AFFILIATION: string (nullable = true)\n",
      " |-- CAND_ELECTION_YR: integer (nullable = true)\n",
      " |-- CAND_OFFICE_ST: string (nullable = true)\n",
      " |-- CAND_OFFICE: string (nullable = true)\n",
      " |-- CAND_OFFICE_DISTRICT: double (nullable = true)\n",
      " |-- CAND_ICI: string (nullable = true)\n",
      " |-- CAND_STATUS: string (nullable = true)\n",
      " |-- CAND_PCC: string (nullable = true)\n",
      " |-- CAND_CITY: string (nullable = true)\n",
      " |-- CAND_ST: string (nullable = true)\n",
      " |-- CAND_ZIP: double (nullable = true)\n",
      " |-- CAND_VOTES: integer (nullable = true)\n",
      " |-- TOTAL_VOTES: integer (nullable = true)\n",
      " |-- PERCENT_VOTES: double (nullable = true)\n",
      " |-- WINNER: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_house.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Incumbent feature column as 1/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|CAND_ICI|count|\n",
      "+--------+-----+\n",
      "|    null|    5|\n",
      "|       O|  198|\n",
      "|       C|  655|\n",
      "|       I|  692|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_house.groupBy('CAND_ICI').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|CAND_ICU|count|\n",
      "+--------+-----+\n",
      "|     0.0|  858|\n",
      "|     1.0|  692|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_house = df_house.withColumn('CAND_ICU', F.when(col('CAND_ICI') == 'I', 1.0).otherwise(0.0))\n",
    "df_house.groupBy('CAND_ICU').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into 2016, 2018 to add in features as they are year dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house16 = df_house.filter(df_house.CAND_ELECTION_YR == 2016)\n",
    "df_house18 = df_house.filter(df_house.CAND_ELECTION_YR == 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2016:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in features 2016\n",
    "avgsum_donation_16 = spark.read.csv('./features/avgsum_donation-16.csv', inferSchema=True, header=True)\n",
    "num_big_donations_16 = spark.read.csv('./features/num_big_donations-16.csv', inferSchema=True, header=True)\n",
    "num_out_of_state_donations_16 = spark.read.csv('./features/num_out_of_state_donations-16.csv', inferSchema=True, header=True)\n",
    "numdonations16 = spark.read.csv('./features/numdonations16.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in features 2018\n",
    "avgsum_donation_18 = spark.read.csv('./features/avgsum_donation-18.csv', inferSchema=True, header=True)\n",
    "num_big_donations_18 = spark.read.csv('./features/num_big_donations-18.csv', inferSchema=True, header=True)\n",
    "num_out_of_state_donations_18 = spark.read.csv('./features/num_out_of_state_donations-18.csv', inferSchema=True, header=True)\n",
    "numdonations18 = spark.read.csv('./features/numdonations18.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2020:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading in features 2020\n",
    "avgsum_donation_20 = spark.read.csv('./features/avgsum_donation-20.csv', inferSchema=True, header=True)\n",
    "num_big_donations_20 = spark.read.csv('./features/num_big_donations-20.csv', inferSchema=True, header=True)\n",
    "num_out_of_state_donations_20 = spark.read.csv('./features/num_out_of_state_donations-20.csv', inferSchema=True, header=True)\n",
    "numdonations20 = spark.read.csv('./features/numdonations20.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Candidates to Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2016:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house16 = df_house16.join(avgsum_donation_16, on='CAND_ID', how='left')\n",
    "df_house16 = df_house16.withColumnRenamed('avgdonation','AVERAGE_DONATION')\n",
    "df_house16 = df_house16.withColumnRenamed('sumdonation','TOTAL_DONATIONS')\n",
    "df_house16 = df_house16.drop(col('_c0'))\n",
    "\n",
    "df_house16 = df_house16.join(num_big_donations_16, on='CAND_ID', how='left')\n",
    "df_house16 = df_house16.withColumnRenamed('numdonat','NUMBER_BIG_DONATIONS')\n",
    "df_house16 = df_house16.drop(col('_c0'))\n",
    "\n",
    "df_house16 = df_house16.join(num_out_of_state_donations_16, on='CAND_ID', how='left')\n",
    "df_house16 = df_house16.withColumnRenamed('numdonat','NUMBER_OUT_OF_STATE_DONATIONS')\n",
    "df_house16 = df_house16.drop(col('_c0'))\n",
    "\n",
    "df_house16 = df_house16.join(numdonations16, on='CAND_ID', how='left')\n",
    "df_house16 = df_house16.withColumnRenamed('numdonat','NUMBER_OF_DONATIONS')\n",
    "df_house16 = df_house16.drop(col('_c0'))\n",
    "\n",
    "#identification based on existence, so filling na values with 0 where none found\n",
    "df_house16 = df_house16.fillna({'NUMBER_BIG_DONATIONS':0, 'NUMBER_OUT_OF_STATE_DONATIONS':0})\n",
    "\n",
    "#not not all candidates were able to join - filter out those without contribution info\n",
    "df_house16 = df_house16.filter(col('TOTAL_DONATIONS').isNotNull())\n",
    "\n",
    "#simpler filters\n",
    "df_house16 = df_house16.withColumn('CONCAT', F.concat(col('CAND_ELECTION_YR'),F.lit('_'),col('CAND_OFFICE_ST'),F.lit('_'),col('CAND_OFFICE_DISTRICT')))\n",
    "\n",
    "#identify races that only have information by CONCAT value\n",
    "single_cand16 = df_house16.groupBy('CAND_ELECTION_YR','CAND_OFFICE_ST','CAND_OFFICE_DISTRICT') \\\n",
    "                          .count() \\\n",
    "                          .filter('count == 1') \\\n",
    "                          .select('CAND_ELECTION_YR','CAND_OFFICE_ST','CAND_OFFICE_DISTRICT') \\\n",
    "                          .withColumn('CONCAT', F.concat(col('CAND_ELECTION_YR'),F.lit('_'),col('CAND_OFFICE_ST'),F.lit('_'),col('CAND_OFFICE_DISTRICT'))) \\\n",
    "                          .select('CONCAT').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#create list of \n",
    "all_cand16 = df_house16.select('CONCAT').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#identifies from all races only those not single\n",
    "multi_cand16 = [x for x in all_cand16 if x not in single_cand16]\n",
    "\n",
    "#filter candidate pool to final form\n",
    "df_house16 = df_house16.filter(col('CONCAT').isin(multi_cand16))\n",
    "\n",
    "\n",
    "#calculate totals for various metrics\n",
    "agg_total_donations16 = df_house16.groupBy('CONCAT').agg({'TOTAL_DONATIONS':'sum'}).withColumnRenamed('sum(TOTAL_DONATIONS)','AGG_TOTAL_DONATIONS')\n",
    "df_house16 = df_house16.join(agg_total_donations16, on='CONCAT', how='left')\n",
    "\n",
    "agg_number_big_donations16 = df_house16.groupBy('CONCAT').agg({'NUMBER_BIG_DONATIONS':'sum'}).withColumnRenamed('sum(NUMBER_BIG_DONATIONS)','AGG_NUMBER_BIG_DONATIONS')\n",
    "df_house16 = df_house16.join(agg_number_big_donations16, on='CONCAT', how='left')\n",
    "\n",
    "agg_number_out_of_state_donations16 = df_house16.groupBy('CONCAT').agg({'NUMBER_OUT_OF_STATE_DONATIONS':'sum'}).withColumnRenamed('sum(NUMBER_OUT_OF_STATE_DONATIONS)','AGG_NUMBER_OUT_OF_STATE_DONATIONS')\n",
    "df_house16 = df_house16.join(agg_number_out_of_state_donations16, on='CONCAT', how='left')\n",
    "\n",
    "agg_number_of_donations16 = df_house16.groupBy('CONCAT').agg({'NUMBER_OF_DONATIONS':'sum'}).withColumnRenamed('sum(NUMBER_OF_DONATIONS)','AGG_NUMBER_OF_DONATIONS')\n",
    "df_house16 = df_house16.join(agg_number_of_donations16, on='CONCAT', how='left')\n",
    "\n",
    "\n",
    "#relative calculations between candidates for a given race\n",
    "df_house16 = df_house16.withColumn('REL_TOTAL_DONATIONS', col('TOTAL_DONATIONS')/col('AGG_TOTAL_DONATIONS'))\n",
    "df_house16 = df_house16.withColumn('REL_NUMBER_BIG_DONATIONS', col('NUMBER_BIG_DONATIONS')/col('AGG_NUMBER_BIG_DONATIONS'))\n",
    "df_house16 = df_house16.withColumn('REL_NUMBER_OUT_OF_STATE_DONATIONS', col('NUMBER_OUT_OF_STATE_DONATIONS')/col('AGG_NUMBER_OUT_OF_STATE_DONATIONS'))\n",
    "df_house16 = df_house16.withColumn('REL_NUMBER_OF_DONATIONS', col('NUMBER_OF_DONATIONS')/col('AGG_NUMBER_OF_DONATIONS'))\n",
    "df_house16 = df_house16.withColumn('PERCENT_BIG_DONATIONS', col('NUMBER_BIG_DONATIONS')/col('NUMBER_OF_DONATIONS'))\n",
    "df_house16 = df_house16.withColumn('PERCENT_OUT_OF_STATE_DONATIONS', col('NUMBER_OUT_OF_STATE_DONATIONS')/col('NUMBER_OF_DONATIONS'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "502"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house16.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2018:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house18 = df_house18.join(avgsum_donation_18, on='CAND_ID', how='left')\n",
    "df_house18 = df_house18.withColumnRenamed('avgdonation','AVERAGE_DONATION')\n",
    "df_house18 = df_house18.withColumnRenamed('sumdonation','TOTAL_DONATIONS')\n",
    "df_house18 = df_house18.drop(col('_c0'))\n",
    "\n",
    "df_house18 = df_house18.join(num_big_donations_18, on='CAND_ID', how='left')\n",
    "df_house18 = df_house18.withColumnRenamed('numdonat','NUMBER_BIG_DONATIONS')\n",
    "df_house18 = df_house18.drop(col('_c0'))\n",
    "\n",
    "df_house18 = df_house18.join(num_out_of_state_donations_18, on='CAND_ID', how='left')\n",
    "df_house18 = df_house18.withColumnRenamed('numdonat','NUMBER_OUT_OF_STATE_DONATIONS')\n",
    "df_house18 = df_house18.drop(col('_c0'))\n",
    "\n",
    "df_house18 = df_house18.join(numdonations18, on='CAND_ID', how='left')\n",
    "df_house18 = df_house18.withColumnRenamed('numdonat','NUMBER_OF_DONATIONS')\n",
    "df_house18 = df_house18.drop(col('_c0'))\n",
    "\n",
    "#identification based on existence, so filling na values with 0 where none found\n",
    "df_house18 = df_house18.fillna({'NUMBER_BIG_DONATIONS':0, 'NUMBER_OUT_OF_STATE_DONATIONS':0})\n",
    "\n",
    "#not not all candidates were able to join - filter out those without contribution info\n",
    "df_house18 = df_house18.filter(col('TOTAL_DONATIONS').isNotNull())\n",
    "\n",
    "#simpler filters\n",
    "df_house18 = df_house18.withColumn('CONCAT', F.concat(col('CAND_ELECTION_YR'),F.lit('_'),col('CAND_OFFICE_ST'),F.lit('_'),col('CAND_OFFICE_DISTRICT')))\n",
    "\n",
    "###\n",
    "\n",
    "#identify races that only have information by CONCAT value\n",
    "single_cand18 = df_house18.groupBy('CAND_ELECTION_YR','CAND_OFFICE_ST','CAND_OFFICE_DISTRICT') \\\n",
    "                          .count() \\\n",
    "                          .filter('count == 1') \\\n",
    "                          .select('CAND_ELECTION_YR','CAND_OFFICE_ST','CAND_OFFICE_DISTRICT') \\\n",
    "                          .withColumn('CONCAT', F.concat(col('CAND_ELECTION_YR'),F.lit('_'),col('CAND_OFFICE_ST'),F.lit('_'),col('CAND_OFFICE_DISTRICT'))) \\\n",
    "                          .select('CONCAT').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#create list of all candidate races\n",
    "all_cand18 = df_house18.select('CONCAT').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#identifies from all races only those not single\n",
    "multi_cand18 = [x for x in all_cand18 if x not in single_cand18]\n",
    "\n",
    "#filter candidate pool to final form\n",
    "df_house18 = df_house18.filter(col('CONCAT').isin(multi_cand18))\n",
    "\n",
    "###\n",
    "\n",
    "#calculate totals for various metrics\n",
    "agg_total_donations18 = df_house18.groupBy('CONCAT').agg({'TOTAL_DONATIONS':'sum'}).withColumnRenamed('sum(TOTAL_DONATIONS)','AGG_TOTAL_DONATIONS')\n",
    "df_house18 = df_house18.join(agg_total_donations18, on='CONCAT', how='left')\n",
    "\n",
    "agg_number_big_donations18 = df_house18.groupBy('CONCAT').agg({'NUMBER_BIG_DONATIONS':'sum'}).withColumnRenamed('sum(NUMBER_BIG_DONATIONS)','AGG_NUMBER_BIG_DONATIONS')\n",
    "df_house18 = df_house18.join(agg_number_big_donations18, on='CONCAT', how='left')\n",
    "\n",
    "agg_number_out_of_state_donations18 = df_house18.groupBy('CONCAT').agg({'NUMBER_OUT_OF_STATE_DONATIONS':'sum'}).withColumnRenamed('sum(NUMBER_OUT_OF_STATE_DONATIONS)','AGG_NUMBER_OUT_OF_STATE_DONATIONS')\n",
    "df_house18 = df_house18.join(agg_number_out_of_state_donations18, on='CONCAT', how='left')\n",
    "\n",
    "agg_number_of_donations18 = df_house18.groupBy('CONCAT').agg({'NUMBER_OF_DONATIONS':'sum'}).withColumnRenamed('sum(NUMBER_OF_DONATIONS)','AGG_NUMBER_OF_DONATIONS')\n",
    "df_house18 = df_house18.join(agg_number_of_donations18, on='CONCAT', how='left')\n",
    "\n",
    "#relative calculations between candidates for a given race\n",
    "df_house18 = df_house18.withColumn('REL_TOTAL_DONATIONS', col('TOTAL_DONATIONS')/col('AGG_TOTAL_DONATIONS'))\n",
    "df_house18 = df_house18.withColumn('REL_NUMBER_BIG_DONATIONS', col('NUMBER_BIG_DONATIONS')/col('AGG_NUMBER_BIG_DONATIONS'))\n",
    "df_house18 = df_house18.withColumn('REL_NUMBER_OUT_OF_STATE_DONATIONS', col('NUMBER_OUT_OF_STATE_DONATIONS')/col('AGG_NUMBER_OUT_OF_STATE_DONATIONS'))\n",
    "df_house18 = df_house18.withColumn('REL_NUMBER_OF_DONATIONS', col('NUMBER_OF_DONATIONS')/col('AGG_NUMBER_OF_DONATIONS'))\n",
    "df_house18 = df_house18.withColumn('PERCENT_BIG_DONATIONS', col('NUMBER_BIG_DONATIONS')/col('NUMBER_OF_DONATIONS'))\n",
    "df_house18 = df_house18.withColumn('PERCENT_OUT_OF_STATE_DONATIONS', col('NUMBER_OUT_OF_STATE_DONATIONS')/col('NUMBER_OF_DONATIONS'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "678"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house18.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine 2016 and 2018:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_house = reduce(DataFrame.unionAll, [df_house16, df_house18])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CONCAT: string (nullable = true)\n",
      " |-- CAND_ID: string (nullable = true)\n",
      " |-- CAND_NAME: string (nullable = true)\n",
      " |-- CAND_PTY_AFFILIATION: string (nullable = true)\n",
      " |-- CAND_ELECTION_YR: integer (nullable = true)\n",
      " |-- CAND_OFFICE_ST: string (nullable = true)\n",
      " |-- CAND_OFFICE: string (nullable = true)\n",
      " |-- CAND_OFFICE_DISTRICT: double (nullable = true)\n",
      " |-- CAND_ICI: string (nullable = true)\n",
      " |-- CAND_STATUS: string (nullable = true)\n",
      " |-- CAND_PCC: string (nullable = true)\n",
      " |-- CAND_CITY: string (nullable = true)\n",
      " |-- CAND_ST: string (nullable = true)\n",
      " |-- CAND_ZIP: double (nullable = true)\n",
      " |-- CAND_VOTES: integer (nullable = true)\n",
      " |-- TOTAL_VOTES: integer (nullable = true)\n",
      " |-- PERCENT_VOTES: double (nullable = true)\n",
      " |-- WINNER: integer (nullable = true)\n",
      " |-- CAND_ICU: double (nullable = false)\n",
      " |-- AVERAGE_DONATION: double (nullable = true)\n",
      " |-- TOTAL_DONATIONS: double (nullable = true)\n",
      " |-- NUMBER_BIG_DONATIONS: integer (nullable = false)\n",
      " |-- NUMBER_OUT_OF_STATE_DONATIONS: integer (nullable = false)\n",
      " |-- NUMBER_OF_DONATIONS: integer (nullable = true)\n",
      " |-- AGG_TOTAL_DONATIONS: double (nullable = true)\n",
      " |-- AGG_NUMBER_BIG_DONATIONS: long (nullable = true)\n",
      " |-- AGG_NUMBER_OUT_OF_STATE_DONATIONS: long (nullable = true)\n",
      " |-- AGG_NUMBER_OF_DONATIONS: long (nullable = true)\n",
      " |-- REL_TOTAL_DONATIONS: double (nullable = true)\n",
      " |-- REL_NUMBER_BIG_DONATIONS: double (nullable = true)\n",
      " |-- REL_NUMBER_OUT_OF_STATE_DONATIONS: double (nullable = true)\n",
      " |-- REL_NUMBER_OF_DONATIONS: double (nullable = true)\n",
      " |-- PERCENT_BIG_DONATIONS: double (nullable = true)\n",
      " |-- PERCENT_OUT_OF_STATE_DONATIONS: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_house.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_house.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Preparation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Inputs for CLASSIFICATION Model:  \n",
    "Defines what variables will be considered and the train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1\n",
    "training_fraction = [0.8, 0.2]\n",
    "ITERS = 5\n",
    "target = 'WINNER'\n",
    "vars_to_keep = ['CAND_ICU','REL_TOTAL_DONATIONS','REL_NUMBER_OF_DONATIONS','PERCENT_BIG_DONATIONS','PERCENT_OUT_OF_STATE_DONATIONS']\n",
    "df_model = df_house.select([target]+vars_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check positive/negative balance of data set:  \n",
    "Relatively balanced, no down/up-sampling required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|WINNER|count|\n",
      "+------+-----+\n",
      "|     1|  560|\n",
      "|     0|  620|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#relatively balanced data set\n",
    "df_model.groupBy(target).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Matrix of the features:\n",
    "Expect to see decent correlation between incumbent status and the funds raised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice to have - not working currently\n",
    "\n",
    "# r=Correlation.corr(df_model, vars_to_keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify **benchmark** classification model by univariate feature selection:  \n",
    "Create simple models of single variables and look at various metrics to determine what will be the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_models(df, target, training_fraction, iters, seed):\n",
    "\n",
    "    # split the data into train/test using seed\n",
    "    data_train, data_test = df.randomSplit(training_fraction,seed=seed)\n",
    "    \n",
    "    # list of predictor variables\n",
    "    vars = df.columns[1:]\n",
    "    \n",
    "    # results storage\n",
    "    df_out = pd.DataFrame(index=vars, columns=['accuracy','precision','recall','auroc'])    \n",
    "\n",
    "    for v in vars:    \n",
    "        # create train and test dataframes with columns: target, v\n",
    "        train = data_train.select(target,v)\n",
    "        test = data_test.select(target,v)\n",
    "\n",
    "        # cast to LabeledPoint\n",
    "        # train\n",
    "        train_lp = train \\\n",
    "                     .rdd \\\n",
    "                     .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "        \n",
    "        # test\n",
    "        test_lp = test \\\n",
    "                     .rdd \\\n",
    "                     .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "\n",
    "        # train logistic regression, setting iterations, including intercept\n",
    "        LR_Model = LogisticRegressionWithLBFGS.train(train_lp, iterations=iters, intercept=True)\n",
    "\n",
    "        pred_test = test_lp.map(lambda p: (p.label, LR_Model.predict(p.features))) \\\n",
    "                           .map(lambda row: (row[0], row[1] * 1.0))      \n",
    "                       \n",
    "        #df_out['weight'].loc[v] = LR_Model.weights  # store the weights\n",
    "\n",
    "        #accuracy = (tp+tn)/all\n",
    "        df_out['accuracy'].loc[v] = 1.0 * pred_test.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "        \n",
    "        tp = pred_test.filter(lambda x: x == (1.0, 1)).count()\n",
    "        fp = pred_test.filter(lambda x: x == (0.0, 1)).count()\n",
    "        fn = pred_test.filter(lambda x: x == (1.0, 0)).count()\n",
    "                \n",
    "        #precision = tp/(tp+fp); essentially of all predicted positive what % correct\n",
    "        df_out['precision'].loc[v] = tp/(tp+fp)                                         \n",
    "       \n",
    "        #recall = tp/(tp+fn); essentially of all condition positive what % correct\n",
    "        df_out['recall'].loc[v] = tp/(tp+fn)\n",
    "        \n",
    "        # metrics.areaUnderROC\n",
    "        metrics = BinaryClassificationMetrics(pred_test)\n",
    "        df_out['auroc'].loc[v] = metrics.areaUnderROC # extract AUROC      \n",
    "        \n",
    "    df_out.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "    \n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.14 s, sys: 437 ms, total: 1.58 s\n",
      "Wall time: 9min 48s\n"
     ]
    }
   ],
   "source": [
    "%time df_output = univariate_models(df_model, target, training_fraction, ITERS, SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CAND_ICU</th>\n",
       "      <td>0.861224</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.871281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REL_TOTAL_DONATIONS</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.793893</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.817999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REL_NUMBER_OF_DONATIONS</th>\n",
       "      <td>0.746939</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.747529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_BIG_DONATIONS</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.532787</td>\n",
       "      <td>0.633077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OUT_OF_STATE_DONATIONS</th>\n",
       "      <td>0.538776</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.303279</td>\n",
       "      <td>0.548504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                accuracy precision    recall     auroc\n",
       "CAND_ICU                        0.861224  0.931373  0.778689  0.871281\n",
       "REL_TOTAL_DONATIONS             0.816327  0.793893  0.852459  0.817999\n",
       "REL_NUMBER_OF_DONATIONS         0.746939  0.734375  0.770492  0.747529\n",
       "PERCENT_BIG_DONATIONS           0.628571  0.656566  0.532787  0.633077\n",
       "PERCENT_OUT_OF_STATE_DONATIONS  0.538776  0.569231  0.303279  0.548504"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this output, **benchmark** classification model is: **Univariate Logistic Regression Model with REL_TOTAL_DONATIONS** as the input variable.  This model exhibits stable, high performance across the evaluation metrics: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild the **benchmark** model to produce the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>105</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0   1\n",
       "0  105  26\n",
       "1   24  90"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "## Classification Benchmark ##\n",
    "##############################\n",
    "\n",
    "train, test = df_house.randomSplit(training_fraction,seed=SEED)\n",
    "    \n",
    "bench_train = train.select(target,'REL_TOTAL_DONATIONS')\n",
    "bench_test = test.select(target,'REL_TOTAL_DONATIONS')\n",
    "\n",
    "bench_train_lp = bench_train \\\n",
    "                .rdd \\\n",
    "                .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "        \n",
    "bench_test_lp = bench_test \\\n",
    "             .rdd \\\n",
    "             .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "\n",
    "bench_LR_Model = LogisticRegressionWithLBFGS.train(bench_train_lp, iterations=ITERS, intercept=True)\n",
    "       \n",
    "bench_pred_test = bench_test_lp.map(lambda p: (p.label, bench_LR_Model.predict(p.features))) \\\n",
    "                               .map(lambda row: (row[0], row[1] * 1.0))\n",
    "\n",
    "################################\n",
    "## Benchmark Confusion Matrix ##\n",
    "################################\n",
    "\n",
    "tp = bench_pred_test.filter(lambda x: x == (1.0, 1)).count()\n",
    "tn = bench_pred_test.filter(lambda x: x == (0.0, 0)).count()\n",
    "fp = bench_pred_test.filter(lambda x: x == (0.0, 1)).count()\n",
    "fn = bench_pred_test.filter(lambda x: x == (1.0, 0)).count()\n",
    "\n",
    "confusion_mtx = pd.DataFrame(data = [[tn,fp],[fn,tp]], index = [0,1], columns = [0,1])\n",
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify **champion** classifcation model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Model implementation candidate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForest Model</th>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.917702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    accuracy  precision    recall     auroc\n",
       "RandomForest Model  0.918367   0.905172  0.921053  0.917702"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "############################\n",
    "## RandomForest Candidate ##\n",
    "############################\n",
    "\n",
    "train, test = df_house.randomSplit(training_fraction,seed=SEED)\n",
    "\n",
    "#including all variables\n",
    "champ_train = train.select([target]+vars_to_keep)\n",
    "champ_test = test.select([target]+vars_to_keep)\n",
    "\n",
    "champ_train_lp = champ_train \\\n",
    "                .rdd \\\n",
    "                .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "        \n",
    "champ_test_lp = champ_test \\\n",
    "             .rdd \\\n",
    "             .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "\n",
    "champ_RF_Model = RandomForest.trainClassifier(champ_train_lp, numClasses=2, categoricalFeaturesInfo={}, \n",
    "                                              numTrees=50, featureSubsetStrategy=\"auto\", \n",
    "                                              impurity='gini', maxDepth=3, maxBins=32, seed=SEED)\n",
    "\n",
    "#significant issues pulling together the labels and estimates - unclear from error readout what the issue was\n",
    "#created workaround to collect and recreate RDD\n",
    "predictions = champ_RF_Model.predict(champ_test_lp.map(lambda x: x.features)).collect()\n",
    "labels = champ_test_lp.map(lambda lp: lp.label).collect()\n",
    "pred_total = list(zip(labels,predictions))\n",
    "champ_test = spark.sparkContext.parallelize(pred_total)\n",
    "\n",
    "\n",
    "######################\n",
    "## Model Evaluation ##\n",
    "######################\n",
    "\n",
    "#accuracy = (tp+tn)/all\n",
    "df_champ1_accuracy = 1.0 * champ_test.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "\n",
    "tp = champ_test.filter(lambda x: x == (1.0, 1)).count()\n",
    "fp = champ_test.filter(lambda x: x == (0.0, 1)).count()\n",
    "fn = champ_test.filter(lambda x: x == (1.0, 0)).count()      \n",
    "        \n",
    "#precision = tp/(tp+fp); essentially of all predicted positive what % correct\n",
    "df_champ1_precision = tp/(tp+fp)                                         \n",
    "       \n",
    "#recall = tp/(tp+fn); essentially of all condition positive what % correct\n",
    "df_champ1_recall = tp/(tp+fn)\n",
    "\n",
    "#auroc\n",
    "champ_metrics = BinaryClassificationMetrics(champ_test)\n",
    "df_champ1_auroc = champ_metrics.areaUnderROC\n",
    "\n",
    "#place values in data frame for presentation\n",
    "df_champ1 = pd.DataFrame(columns = ['accuracy','precision','recall','auroc'])\n",
    "df_champ1.loc[0] = [df_champ1_accuracy,df_champ1_precision,df_champ1_recall,df_champ1_auroc]\n",
    "df_champ1 = df_champ1.rename(index={0:'RandomForest Model'})\n",
    "df_champ1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  120   11\n",
       "1    9  105"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#########################\n",
    "## RF Confusion Matrix ##\n",
    "#########################\n",
    "\n",
    "tp = champ_test.filter(lambda x: x == (1.0, 1)).count()\n",
    "tn = champ_test.filter(lambda x: x == (0.0, 0)).count()\n",
    "fp = champ_test.filter(lambda x: x == (0.0, 1)).count()\n",
    "fn = champ_test.filter(lambda x: x == (1.0, 0)).count()\n",
    "\n",
    "confusion_mtx = pd.DataFrame(data = [[tn,fp],[fn,tp]], index = [0,1], columns = [0,1])\n",
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Gradient-Boosted Tree implementation candidate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gradient-Boosted Tree Model</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.87069</td>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.884957</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             accuracy  precision    recall     auroc\n",
       "Gradient-Boosted Tree Model  0.885714    0.87069  0.885965  0.884957"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##############################\n",
    "## Gradient Boost Candidate ##\n",
    "##############################\n",
    "\n",
    "train, test = df_house.randomSplit(training_fraction,seed=SEED)\n",
    "\n",
    "#including all variables\n",
    "champ2_train = train.select([target]+vars_to_keep)\n",
    "champ2_test = test.select([target]+vars_to_keep)\n",
    "\n",
    "champ2_train_lp = champ2_train \\\n",
    "                .rdd \\\n",
    "                .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "        \n",
    "champ2_test_lp = champ2_test \\\n",
    "             .rdd \\\n",
    "             .map(lambda row: reg.LabeledPoint(row[0], row[1:]))\n",
    "\n",
    "champ_GBT_Model = GradientBoostedTrees.trainClassifier(champ2_train_lp, categoricalFeaturesInfo={}, \n",
    "                                                       numIterations=50, learningRate=0.1, \n",
    "                                                       maxDepth=3, maxBins=32)\n",
    "\n",
    "#significant issues pulling together the labels and estimates - unclear from error readout what the issue was\n",
    "#created workaround to collect and recreate RDD\n",
    "predictions = champ_GBT_Model.predict(champ2_test_lp.map(lambda x: x.features)).collect()\n",
    "labels = champ2_test_lp.map(lambda lp: lp.label).collect()\n",
    "pred_total = list(zip(labels,predictions))\n",
    "champ2_test = spark.sparkContext.parallelize(pred_total)\n",
    "\n",
    "######################\n",
    "## Model Evaluation ##\n",
    "######################\n",
    "\n",
    "#accuracy = (tp+tn)/all\n",
    "df_champ2_accuracy = 1.0 * champ2_test.filter(lambda pl: pl[0] == pl[1]).count() / test.count()\n",
    "\n",
    "tp = champ2_test.filter(lambda x: x == (1.0, 1)).count()\n",
    "fp = champ2_test.filter(lambda x: x == (0.0, 1)).count()\n",
    "fn = champ2_test.filter(lambda x: x == (1.0, 0)).count()      \n",
    "        \n",
    "#precision = tp/(tp+fp); essentially of all predicted positive what % correct\n",
    "df_champ2_precision = tp/(tp+fp)                                         \n",
    "       \n",
    "#recall = tp/(tp+fn); essentially of all condition positive what % correct\n",
    "df_champ2_recall = tp/(tp+fn)\n",
    "\n",
    "#auroc\n",
    "champ2_metrics = BinaryClassificationMetrics(champ2_test)\n",
    "df_champ2_auroc = champ2_metrics.areaUnderROC\n",
    "\n",
    "#place values in data frame for presentation\n",
    "df_champ2 = pd.DataFrame(columns = ['accuracy','precision','recall','auroc'])\n",
    "df_champ2.loc[0] = [df_champ2_accuracy,df_champ2_precision,df_champ2_recall,df_champ2_auroc]\n",
    "df_champ2 = df_champ2.rename(index={0:'Gradient-Boosted Tree Model'})\n",
    "df_champ2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>116</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  116   15\n",
       "1   13  101"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##########################\n",
    "## GBT Confusion Matrix ##\n",
    "##########################\n",
    "\n",
    "tp = champ2_test.filter(lambda x: x == (1.0, 1)).count()\n",
    "tn = champ2_test.filter(lambda x: x == (0.0, 0)).count()\n",
    "fp = champ2_test.filter(lambda x: x == (0.0, 1)).count()\n",
    "fn = champ2_test.filter(lambda x: x == (1.0, 0)).count()\n",
    "\n",
    "confusion_mtx = pd.DataFrame(data = [[tn,fp],[fn,tp]], index = [0,1], columns = [0,1])\n",
    "confusion_mtx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results/Conclusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize Classification Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>auroc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RandomForest Model</th>\n",
       "      <td>0.918367</td>\n",
       "      <td>0.905172</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.917702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gradient-Boosted Tree Model</th>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.87069</td>\n",
       "      <td>0.885965</td>\n",
       "      <td>0.884957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CAND_ICU</th>\n",
       "      <td>0.861224</td>\n",
       "      <td>0.931373</td>\n",
       "      <td>0.778689</td>\n",
       "      <td>0.871281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REL_TOTAL_DONATIONS</th>\n",
       "      <td>0.816327</td>\n",
       "      <td>0.793893</td>\n",
       "      <td>0.852459</td>\n",
       "      <td>0.817999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REL_NUMBER_OF_DONATIONS</th>\n",
       "      <td>0.746939</td>\n",
       "      <td>0.734375</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.747529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_BIG_DONATIONS</th>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.656566</td>\n",
       "      <td>0.532787</td>\n",
       "      <td>0.633077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PERCENT_OUT_OF_STATE_DONATIONS</th>\n",
       "      <td>0.538776</td>\n",
       "      <td>0.569231</td>\n",
       "      <td>0.303279</td>\n",
       "      <td>0.548504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                accuracy precision    recall     auroc\n",
       "RandomForest Model              0.918367  0.905172  0.921053  0.917702\n",
       "Gradient-Boosted Tree Model     0.885714   0.87069  0.885965  0.884957\n",
       "CAND_ICU                        0.861224  0.931373  0.778689  0.871281\n",
       "REL_TOTAL_DONATIONS             0.816327  0.793893  0.852459  0.817999\n",
       "REL_NUMBER_OF_DONATIONS         0.746939  0.734375  0.770492  0.747529\n",
       "PERCENT_BIG_DONATIONS           0.628571  0.656566  0.532787  0.633077\n",
       "PERCENT_OUT_OF_STATE_DONATIONS  0.538776  0.569231  0.303279  0.548504"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_all = pd.concat([df_champ2, df_champ1, df_output])\n",
    "results_all.sort_values(by='accuracy', ascending=False, inplace=True)\n",
    "results_all"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS 5559",
   "language": "python",
   "name": "ds5559"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
